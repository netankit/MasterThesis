\chapter{Sentiment Analysis}\label{chapter:sentiment}
Till now we have studied about Sentiment Analysis with a general perspective. This chapter starts a more computational view of solving this problem. Before we begin discussing various methods, its important to understand the scope of the problem. 
\section{Sentiment Analysis Levels}
The sentiment analysis problem is not a single view problem, rather its can be viewed as multi-level problem. They are discussed as follows:
\begin{itemize}
\item \textbf{Document Level: }Identifying if the \textit{document} (product review, blog-post, forum post etc.) expresses opinions and if they do, classifying them as positive, negative or neutral. this takes into account the overall sentiment expressed by the opinion holder for the whole document.  
\item \textbf{Sentence Level: }Identifying whether a single \textit{sentence} is positive, negative or neutral.
\item \textbf{Attribute Level: }Extracting the \textit{object attributes} (image quality, zoom size etc.) that are subject of an opinion and the opinion orientation.

It is important to note here that as the object becomes more granular, the intensity or difficulty of sentiment analysis increases.
\end{itemize}

\section{Document-Level Sentiment Analysis}
We start with the traditional methods for document analysis sentiment analysis. It's important to keep in mind few assumptions which come along the way. 
\begin{itemize}
\item The document is opinionated on a single object
\item the opinions are from a single opinion holder.
\end{itemize}
In real world data, these assumptions don't usually hold. But, for the sake of simplicity, we start with them to make the understanding of the methods easy.

Also, more importantly,  this analysis is slightly different from the topic based text classification. In topic classification, heavy emphasis is given to the extraction of the topics. Thus, the \textit{topic words} become very important.  Whereas, in sentiment classification, the opinion words are more important. For example: wonderful, fabulous and terrible. 

So to start with, a naive sentiment classifier can be based on identification and processing of these opinion words. Opinionated words are also known as sentiment words, opinion lexicon, polarity words or opinion bearing words. They are categorized into two types. First, \textbf{Base types}, for example:

Positive: wonderful, elegant, amazing

Negative: horrible disgusting poor

Second, Comparative Types, for example: better, worse etc.

\subsection{Counting Opinion Words}

We start with a simple method. We try to count the opinion words in a given document. We are given an opinion word list which have both positive and negative words. And we assign an \textbf{orientation score} (-1,1) to all words in this list. Such that, \newline

Positive opinion words (+1): great, amazing, love ...

Negative opinion words (-1): hate, horrible, etc ...

Strength of these words can also be defined [0,1] 
\newline

Thus the orientation score for the document, is the sum of the orientation scores of all opinion words found. Lets, take an example review and count the opinion words in the same and calculate overall orientation score for that review. Example:

“My Canon Powershot takes \textbf{great} pictures! ... My friend had gotten one about a year ago and she \textbf{loves} it. So, after seeing her enthusiasm about it I decided to get one and I will never go back to any other camera. I absolutely \textbf{love} this camera. I believe that every person on Earth should own one of these. ... It is \textbf{amazing}! ... There is not one thing I \textbf{hate} about this product, which is strange because I am a very picky person! ...”

Positive Sentiment Words: great (1), love (2) and amazing (1); Total: 4

Negative Sentiment Words: hate (1); Total: 1
\newline 

Overall Sentiment Orientation for Review = 4-1 = 3 or \textbf{Positive Review Overall}!

After looking at this analysis, lets ask ourselves a question. \textbf{Is simply counting the opinion words good enough?}

The answer to that is \textbf{NO}! Lets' see why. Just take into consideration the last sentence in the above review. 

There is \textbf{not} one thing I \textbf{hate} about this product. $\rightarrow$ Not Negative!
\newline

This is positive sentence overall. Since, we only captured our negative sentiment word \textbf{"hate"}, we labeled this sentence as negative, which is not true. Because of the negation indicator "not", the sentiment is reversed as in  \textbf{not ... hate} $\Rightarrow$ \textbf{like}.. Thus, an important lesson here is that we need to handle negation.  A common pattern of analysis is:
\newline

"not ... \textbf{negative}" $\rightarrow$ \textbf{positive}

"never ... \textbf{negative}" $\rightarrow$ \textbf{positive}
\newline

Thus, the overall orientation score of the review is 4 +1 = 5 and not 3 as stated previously.
\newline

\textbf{A word of caution:} Negation must be handled very carefully! As, "not" in the pattern: "not only ... but also" does not change the orientation. 

\subsection{Rule Based Method}

Another method to compute the overall sentiment of a document is through handcrafted rules which are provided by a trained linguist, based on the identification of linguistic phenomenon which determine sentiment for a given language. We first discuss some related terminology:
\begin{itemize}
\item \textbf{Pattern / Rule:} A sequence of tokens.
\item \textbf{Token:} An abstraction of a word, represented using lemma, polarity tag or a part of speech tag. There are two special tokens. TOPIC: an attribute (ex. weight, size etc.) and GAP\_DIGIT\_DIGIT: How many words can be skipping between two tokens to allow more tolerant matching. Ex GAP\_1\_2 
\item \textbf{Polarity Tag:} positive, negative, neutral, NOT (negation)
\item \textbf{POS Tag:} NN (Noun), VB (Verb), JJ (Adjective), RB (Adverb), IN (preposition) ...
\end{itemize}

Examples of some basic opinion rules, Label Sequential Pattern (LSP) Matching:
\begin{itemize}
\item Subject {like | adore | want | work} TOPIC $\rightarrow$ positive, e.g. – ``I like the old camera”
\item Subject {is | are} {great | fantastic | simple | easy} $\rightarrow$ positive, e.g. – ``This camera is fantastic”
\item TOPIC GAP\_0\_3 NOT work $\rightarrow$ negative, e.g. – ``The new search still does not work”
\item Please do NOT VB $\rightarrow$ negative, e.g. – ``Please do not roll out this new search!”
\item NOT GAP\_0\_3 {want | think | believe | need | get} $\rightarrow$ negative, e.g. – ``I do not want large size pictures in the Gallery window.”
\item {get | bring | give | put | change} GAP\_0\_3 TOPIC GAP\_0\_3 back $\rightarrow$ positive
– ``Please put the old search and browse back!”
\end{itemize} 

Although this method seems to work pretty well, but there is a fundamental limitation in putting it to use at large scale. Any rule based mining methods are computationally expensive. In this case, only a limited number of opinionated words are found and only a limited number of patterns can be created. 

Note: There are methods, which are helpful in generation and classification of these sentiment words automatically, but we do not go deeper into the same. For the more curious, some methods which are used for this purpose are discussed in the works of Turney 2002 which make use of PMI (Point-wise Mutual Information), Dictionary based methods (SentiWordnet), Corpus based Methods (which rely on syntactic or co-occurrence patterns in large text corpora) etc. 

TODO: Refer several related papers here! 

\subsection{Supervised Machine Learning Techniques}
So, far we have seen simple count based and rule based methods. Although these methods are simple to comprehend and implement,  they don't scale well for large corpora. So, to achieve an overall improvement, we make use of machine learning. What machine learning allows is to help find the patterns in known set of documents and then apply those patterns learned on new documents. For the case of supervise learning, we require training and testing datasets. And a set of features to represent documents. 
\newline

In the case of sentiment analysis the final learning goal is to classify sentence into positive, negative  or neutral. A very common way to try out this method is on product reviews dataset. In product reviews, we find that the users review by selecting starts [1,5] or thumbs up or down, where 5 or thumbs up means very good rating and 1 or thumbs down represents very low rating.  This can be used his review as positive or negative and use the same as training data. We can then perform sentiment analysis on those reviews which have not been specifically rated by the user. 
\newline
An important step in the process is representing these reviews in such a way that its consistent and they are also understood by computers alike. This begins with something known as "Feature Extraction". Some general methods are discussed below:
\begin{itemize}
\item \textbf{Terms and Frequency}
\begin{itemize}
\item Unigram and more general n-grams.
\item Word Position Information
\item Term Frequency - Inverse Document Frequency (TF-IDF) weighting.
\end{itemize}
\item \textbf{Part of Speech Tags:} Adjectives are usually important indicators of subjectivity and opinions. 
\item \textbf{Syntactic Dependencies:} The syntactic structure of a sentence can be captured by representing the sentence as a \textbf{Syntax Parse Tree}. See~\autoref{fig:parsetree}. 

\begin{figure}[htb]
  \centering
  \includegraphics{figures/2_parsetree1}
  \caption[Parse Tree Emaple]{Parse Tree for sentence "John hit the ball." }\label{fig:parsetree}
\end{figure}
\end{itemize} 

Once we have perfromed feature extraction, then the next step is to feed the feature input to the machine learning algorithm. There are a number of very successful and efficient supervised machine learning methods. They are described briefly as follows:
\begin{itemize}
\item \textbf{Na\"{i}ve Bayes:} A simple probabilistic classifier based on applying the Bayes' theorem with strong (naive) independence assumption.
\item \textbf{Maximum Entropy (ME):}  A probabilistic model that estimates the conditional distribution of the class label
\item \textbf{Support Vector Machines (SVM)} [Pang et al, EMNLP2002] A representation of the examples as points in space in which support vectors are computed to provide a best division of points/examples into categories
\item \textbf{Logistic Regression (LR) Model} [Pang \& Lee, ACL 2005] A LR model predicts the classes from a set of variables that may be continuous, discrete, or a mixture.
\end{itemize}

Since, the topics of this work is focused on exploring unsupervised word representations learned via deep learning and its application to the problem of sentiment analysis, we will not expand this discussion further.
\subsection{Domain Adaptation}
